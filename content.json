{"pages":[],"posts":[{"title":"统计推断中的参数估计理论","text":"该部分内容对应Hogg课本的第6-8章，主要讨论参数估计的几种常用方法。 一、参数的点估计设随机样本$X_i \\sim f(x;\\theta)$，其中$\\theta \\in \\Omega$，我们期望构造统计量$Y_1=u_1(X_1,X_2,\\cdots,X_n)$对参数$\\theta$有良好的估计，即寻找参数$\\theta$的点估计，这主要通过两种方法：最大似然估计和矩法估计，且它们分别对应不同的准则. 概念1: 最大似然估计从概率论的观点看，小概率事件在一次试验中是几乎不发生的，即如果在一次试验中事件$A$发生了，我们则认为$p(A)$较大. 在这一准则下，参数的取法应使概率密度（即事件发生的可能性）最大，于是我们引出最大似然估计的定义： 定义1. 参数$\\theta$的最大似然估计(maximum likelihood estimate)记为$\\hat{\\theta}_L$，它满足 $$\\hat{\\theta}_L = \\arg\\max_{\\theta \\in \\Omega} L(x_1,x_2,\\cdots,x_n;\\theta) = \\arg\\max_{\\theta \\in \\Omega} \\ln L(x_1,x_2,\\cdots,x_n;\\theta)$$ 其中$L(x_1,x_2,\\cdots,x_n;\\theta)=\\prod f(x_i;\\theta)$称为$\\theta$的似然函数(likelihood function). 性质1. 最大似然估计具有不变性(invariance property)，即对于可逆函数$h$，$\\eta=h(\\theta)$的最大似然估计为 $$\\hat{\\eta}_L=\\widehat{h(\\theta)}_L=h(\\hat{\\theta}_L)$$ 例1 (ex 6.1). 求下列密度函数中$\\theta$的最大似然估计量： (a) $f(x;\\theta)=\\frac{1}{\\theta} e^{-\\frac{x}{\\theta}},0&lt;x&lt;\\infty,0&lt;\\theta&lt;\\infty$； (b) $f(x;\\theta)=\\frac{1}{2}e^{-|x-\\theta|},-\\infty&lt;x&lt;\\infty,-\\infty&lt;\\theta&lt;\\infty$； (c) $f(x;\\theta)=e^{-(x-\\theta)},\\theta \\leq x &lt; \\infty,-\\infty&lt;\\theta&lt;\\infty$. 解: (a) 似然函数为$L(\\theta)=\\prod f(x_i;\\theta)=\\theta^{-n} e^{-\\frac{1}{\\theta}\\sum x_i}$，由 $$\\frac{\\partial \\ln L(\\theta)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left[ -n\\ln \\theta-\\frac{1}{\\theta}\\sum_{i=1}^n x_i \\right] = -\\frac{n}{\\theta} + \\frac{1}{\\theta^2} \\sum_{i=1}^n x_i = 0$$ 解得$\\hat{\\theta}_L = \\frac{1}{n}\\sum X_i = \\overline{X}$. (b) 似然函数为$L(\\theta)=\\prod f(x_i;\\theta)=2^{-n} e^{-\\sum|x_i-\\theta|}$，取对数得 $$\\ln L(\\theta)=-n \\ln 2-\\sum_{i=1}^n |x_i-\\theta|$$ 则$\\ln L(\\theta)$取最大值当且仅当$\\theta$取$x_1,x_2,\\cdots,x_n$的中位数，所以$\\hat{\\theta}_L$为$X_1,X_2,\\cdots,X_n$的中位数. (c) 似然函数为$L(\\theta)=\\prod f(x_i;\\theta)=e^{n\\theta-\\sum x_i}$，由 $$\\frac{\\partial \\ln L(\\theta)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left[ n\\theta-\\sum_{i=1}^n x_i \\right]=n&gt;0$$ 知似然函数$L(\\theta)$为$\\theta$的单增函数，所以$\\hat{\\theta}_L = \\min(X_i)$. $\\square$ 概念2: 矩法估计首先我们给出辛钦大数定律的推广形式：设$X_1,X_2,\\cdots,X_n$相互独立，与随机变量$X \\sim f(x;\\theta)$同分布，且$E[|X|^k]&lt;\\infty$，则$\\forall \\varepsilon &gt; 0$，有 $$\\lim_{n \\to \\infty} P \\left( \\left| \\frac{1}{n}\\sum_{i=1}^n X_i^k - E[X^k] \\right| \\geq \\varepsilon \\right) = 0$$ 以此为准则，我们得到两种形式的矩法估计： 用样本$k$阶原点矩$\\overline{X^k}=\\sum_{i=1}^n X_i^k$估计总体$k$阶原点矩$E[X^k]$； 用样本$k$阶中心矩$\\sum\\limits_{i=1}^n (X_i-\\overline{X})^k$估计总体$k$阶中心矩$E[(X-E[X])^k]$. 其中最常用的两类矩法估计为： 用样本均值$\\overline{X}=\\frac{1}{n}\\sum\\limits_{i=1}^n X_i$估计总体均值$E[X]$； 用样本方差$S_n^2=\\frac{1}{n}\\sum\\limits_{i=1}^n(X_i-\\overline{X})^2$估计总体方差$\\operatorname{var}[X]$. 性质2. 根据辛钦大数定律，参数$\\theta$的矩法估计一定为相合估计，相合估计的具体定义将在本节的最后给出. 例2 (ex 6.7). 设$X \\sim f(x;\\theta)=\\theta x^{\\theta-1},0&lt;x&lt;1,0&lt;\\theta&lt;\\infty$，求$\\theta$的矩法估计$\\hat{\\theta}_M$，并证明$\\hat{\\theta}_M$为$\\theta$的相合估计. 解: 计算得总体均值为$E[X]=\\int_0^1 x \\cdot \\theta x^{\\theta-1} \\mathrm{d}x=\\frac{\\theta}{\\theta+1}$. 用样本均值估计总体均值： $$E[X]=\\frac{\\theta}{\\theta+1}=\\frac{1}{n}\\sum_{i=1}^n X_i=\\overline{X}$$ 故$\\hat{\\theta}_M=\\frac{\\overline{X}}{1-\\overline{X}}$. 由强大数定律可知：$\\overline{X} \\stackrel{\\text{a.s.}}{\\rightarrow} E[X]$，于是 $$\\hat{\\theta}_M=\\frac{\\overline{X}}{1-\\overline{X}} \\stackrel{\\text{a.s.}}{\\rightarrow} \\frac{E[X]}{1-E[X]}=\\theta \\quad (n \\to \\infty) \\Rightarrow \\hat{\\theta}_M \\stackrel{p}{\\rightarrow} \\theta$$ 即$\\hat{\\theta}_M$为$\\theta$的相合估计. $\\square$ 二、参数的区间估计设$X \\sim f(x;\\theta)$，对给定置信度$1-\\alpha(0&lt;\\alpha \\ll 1)$，区间估计要求我们找到统计量$T_1(X_1,X_2,\\cdots,X_n)$和$T_2(X_1,X_2,\\cdots,X_n)$，满足 $$P(T_1 \\leq \\theta \\leq T_2) = 1-\\alpha$$ 其中$[T_1,T_2]$称为$\\theta$的置信度为$1-\\alpha$的置信区间(confidence interval). 下面我们将重点放在正态总体参数的区间估计上. 概念1: 抽样分布基本定理定理1 (抽样分布基本定理). 设$X \\sim N(\\mu,\\sigma^2)$，$(X_1,X_2,\\cdots,X_n)$是$X$的样本，则 (1) $\\overline{X}$与$S_n^2$独立； (2) $\\frac{nS_n^2}{\\sigma^2} \\sim \\chi^2(n-1), \\frac{n(\\overline{X}-\\mu)^2}{\\sigma^2} \\sim \\chi^2(n)$. 推论2. 设$X \\sim N(\\mu,\\sigma^2)$，则$\\sqrt{n-1}\\frac{\\overline{X}-\\mu}{S_n} \\sim t(n-1)$. 证明: $X \\sim N(\\mu,\\sigma^2) \\Rightarrow \\overline{X} \\sim N(\\mu,\\sigma^2/n)$. 由抽样分布基本定理知$\\overline{X}$与$S_n^2$独立，且$\\frac{nS_n^2}{\\sigma^2} \\sim \\chi^2(n-1)$，从而 $$\\frac{\\frac{\\overline{X}-\\mu}{\\sqrt{\\sigma^2/n}}}{\\sqrt{\\frac{nS_n^2}{\\sigma^2(n-1)}}}=\\sqrt{n-1}\\frac{\\overline{X}-\\mu}{S_n} \\sim t(n-1)$$ 推论3. 设$X \\sim N(\\mu_1,\\sigma_1^2),Y \\sim N(\\mu_2,\\sigma_2^2)$，$X$与$Y$独立且$\\sigma_1^2=\\sigma_2^2=\\sigma^2$. 设$(X_1,X_2,\\cdots,X_m)$是$X$的样本，记$\\overline{X}=\\frac{1}{m}\\sum\\limits_{i=1}^m X_i$，$S_{1m}^2=\\frac{1}{m}\\sum\\limits_{i=1}^m (X_i-\\overline{X})^2$； 设$(Y_1,Y_2,\\cdots,Y_n)$是$Y$的样本，记$\\overline{Y}=\\frac{1}{n}\\sum\\limits_{i=1}^n Y_i$，$S_{2n}^2=\\frac{1}{n}\\sum\\limits_{i=1}^n (Y_i-\\overline{Y})^2$. 则$T=\\dfrac{(\\overline{X}-\\overline{Y})-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{1}{m}+\\frac{1}{n}}\\sqrt{\\frac{mS_{1m}^2+nS_{2n}^2}{m+n-2}}} \\sim t(m+n-2)$. 证明: 由于$X \\sim N(\\mu_1,\\sigma_1^2),Y \\sim N(\\mu_2,\\sigma_2^2)$，所以 $$\\overline{X}-\\overline{Y} \\sim N \\left(\\mu_1-\\mu_2,\\frac{\\sigma_1^2}{m}+\\frac{\\sigma_2^2}{n} \\right)$$ 由抽样分布基本定理知：$\\frac{mS_{1m}^2}{\\sigma_1^2} \\sim \\chi^2(m-1),\\frac{nS_{2n}^2}{\\sigma_2^2} \\sim \\chi^2(n-1)$且二者独立，所以 $$\\frac{mS_{1m}^2}{\\sigma_1^2}+\\frac{nS_{2n}^2}{\\sigma_2^2} \\sim \\chi^2(m+n-2)$$ 结合$\\sigma_1^2=\\sigma_2^2=\\sigma^2$则有 $$T=\\frac{\\frac{(\\overline{X}-\\overline{Y})-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{m}+\\frac{\\sigma_2^2}{n}}}}{\\sqrt{\\frac{\\frac{mS_{1m}^2}{\\sigma_1^2}+\\frac{nS_{2n}^2}{\\sigma_2^2}}{m+n-2}}}=\\frac{(\\overline{X}-\\overline{Y})-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{1}{m}+\\frac{1}{n}}\\sqrt{\\frac{mS_{1m}^2+nS_{2n}^2}{m+n-2}}} \\sim t(m+n-2)$$ 推论4. 设$X \\sim N(\\mu_1,\\sigma_1^2),Y \\sim N(\\mu_2,\\sigma_2^2)$，$X$与$Y$独立，则 $$F=\\frac{mS_{1m}^2 (n-1)}{nS_{2n}^2(m-1)} \\cdot \\frac{\\sigma_2^2}{\\sigma_1^2} \\sim F(m-1,n-1)$$ 证明: 由抽样分布基本定理知$\\frac{mS_{1m}^2}{\\sigma_1^2} \\sim \\chi^2(m-1),\\frac{nS_{2n}^2}{\\sigma_2^2} \\sim \\chi^2(n-1)$且二者独立，则 $$F=\\frac{\\frac{mS_{1m}^2}{\\sigma_1^2(m-1)}}{\\frac{nS_{2n}^2}{\\sigma_2^2(n-1)}}=\\frac{mS_{1m}^2 (n-1)}{nS_{2n}^2(m-1)} \\cdot \\frac{\\sigma_2^2}{\\sigma_1^2} \\sim F(m-1,n-1)$$ 概念2: 正态总体参数的区间估计对于区间估计，只需找到包含参数的统计量且服从标准正态分布、$t$分布、$\\chi^2$分布或$F$分布，就可以通过这些分布的分位数构造出相应的置信区间，下面分别对单个总体的参数$\\mu,\\sigma^2$以及两个总体的参数$\\mu_1-\\mu_2,\\frac{\\sigma_2^2}{\\sigma_1^2}$构造区间估计所需的统计量，它们的构造全部基于上述的抽样分布基本定理. 单个总体关于参数$\\mu$： $\\sigma^2=\\sigma_0^2$已知：$\\frac{\\overline{X}-\\mu}{\\sqrt{\\sigma_0^2/n}} \\sim N(0,1)$ $\\sigma^2$未知：$\\sqrt{n-1}\\frac{\\overline{X}-\\mu}{S_n} \\sim t(n-1)$ 关于参数$\\sigma^2$： $\\mu=\\mu_0$已知：$\\frac{\\sum (X_i-\\mu)^2}{\\sigma^2} \\sim \\chi^2(n)$ $\\mu$未知：$\\frac{nS_n^2}{\\sigma^2} \\sim \\chi^2(n-1)$ 两个总体关于参数$\\mu_1-\\mu_2$： $\\sigma_1^2,\\sigma_2^2$已知：$\\frac{(\\overline{X}-\\overline{Y})-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{m}+\\frac{\\sigma_2^2}{n}}} \\sim N(0,1)$ $\\sigma_1^2=\\sigma_2^2=\\sigma^2$未知：$\\frac{(\\overline{X}-\\overline{Y})-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{1}{m}+\\frac{1}{n}}\\sqrt{\\frac{mS_{1m}^2+nS_{2n}^2}{m+n-2}}} \\sim t(m+n-2)$ 关于参数$\\frac{\\sigma_2^2}{\\sigma_1^2}$： $\\mu_1,\\mu_2$已知：$\\frac{n \\sum (X_i-\\mu_1)^2}{m \\sum (Y_j-\\mu_2)^2} \\cdot \\frac{\\sigma_2^2}{\\sigma_1^2} \\sim F(m,n)$ $\\mu_1,\\mu_2$未知：$\\frac{mS_{1m}^2 (n-1)}{nS_{2n}^2(m-1)} \\cdot \\frac{\\sigma_2^2}{\\sigma_1^2} \\sim F(m-1,n-1)$ 三、参数估计的优劣评价对于参数$\\theta$的点估计$\\hat{\\theta}$，我们需要引入一套评价标准来评判点估计的优劣，它包括：无偏性、相合性、有效性；而正是优劣评价标准的引入，促使我们去寻求参数“最好的点估计”，即一致最小方差无偏估计(UMVUE)，在下一节中我们将详细探讨寻找一致最小方差无偏估计的方法. 概念1: 无偏性定义2. 设$Y_1=u_1(X_1,X_2,\\cdots,X_n)$，称$Y_1$为$\\theta$的无偏估计量(unbiased estimator)，如果$E[Y_1]=\\theta$. 不然，称$Y_1$为$\\theta$的有偏估计量(biased estimator). 概念2: 相合性定义3. 设$Y_1=u_1(X_1,X_2,\\cdots,X_n)$，称$Y_1$为$\\theta$的相合估计量(consistent estimator)，如果$Y_1 \\stackrel{p}{\\to} \\theta$，即对$\\forall \\epsilon &gt; 0$，有$\\lim\\limits_{n\\to\\infty} P(|Y_1-\\theta| \\geq \\varepsilon)=0$. 概念3: 有效性定义4. 设$Y_1,Y_2$为$\\theta$的无偏估计量，即$E[Y_1]=E[Y_2]=\\theta$，称$Y_1$比$Y_2$更有效(efficient)，如果$\\operatorname{var}[Y_1]&lt;\\operatorname{var}[Y_2]$. 参考文献 Hogg, R. V., &amp; Craig, A. T. (. T. (1995). Introduction to mathematical statistics (5th ed.). Englewood Cliffs, N.J.: Prentice Hall.","link":"/2020/01/12/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/Parameter_Estimation/"},{"title":"求一致最小方差无偏估计的方法","text":"该部分内容对应Hogg课本的第7-8章，主要讨论构造一致最小方差无偏估计(UMVUE)的两种思路，一种是通过构造充分完备统计量的函数，另一种则是通过构造有效估计量。 一、充分完备统计量概念1: 充分统计量定义1. 称$Y_1=u(X_1,X_2,\\cdots,X_n)$为$\\theta$的充分统计量(sufficient statistic)，如果 $$\\frac{f(x_1;\\theta)f(x_2;\\theta) \\cdots f(x_n;\\theta)}{g_1(u(x_1,x_2,\\cdots,x_n);\\theta)} = H(x_1,x_2,\\cdots,x_n)$$ 不依赖于$\\theta \\in \\Omega$. 例1. 设$X \\sim \\Gamma(2,\\theta)$，$Y_1=X_1+X_2+\\cdots+X_n$，证明$Y_1$是关于$\\theta$的充分统计量. 证明: 由于$X$的矩母函数为$M_X(t)=E[e^{tX}]=(1-\\theta t)^{-2}, t&lt;1/\\theta$，所以 $$M_{Y_1}(t)=E[e^{t\\sum X_i}]=(1-\\theta t)^{-2n} \\Rightarrow Y_1 \\sim \\Gamma(2n,\\theta)$$ 根据以上定义， $$\\begin{aligned}\\frac{f(x_1;\\theta)f(x_2;\\theta) \\cdots f(x_n;\\theta)}{g_1(y_1;\\theta)} &amp;= \\frac{\\prod\\limits_{i=1}^n \\frac{1}{\\Gamma(2)\\theta^2} x_ie^{-x_i/\\theta}}{\\frac{1}{\\Gamma(2n)\\theta^{2n}}(\\sum x_i)^{2n-1}e^{-\\sum x_i/\\theta}} \\\\\\&amp;=\\frac{\\Gamma(2n)}{\\Gamma(2)^n}\\frac{\\prod_{i=1}^n x_i}{(\\sum x_i)^{2n-1}}\\end{aligned}$$ 等式右端不依赖于$\\theta$，所以$Y_1$是关于$\\theta$的充分统计量. $\\square$ 定理1 (Factorization Thm). $Y_1=u(X_1,X_2,\\cdots,X_n)$为$\\theta$的充分统计量 $\\Leftrightarrow$ 存在非负函数$k_1,k_2$使得 $$f(x_1;\\theta)f(x_2;\\theta) \\cdots f(x_n;\\theta)=k_1[u(x_1,x_2,\\cdots,x_n);\\theta] \\cdot k_2(x_1,x_2,\\cdots,x_n)$$ 其中$k_2$不依赖于$\\theta$. 例2. 题设同例1，利用因子分解定理： $$\\begin{aligned}f(x_1;\\theta)f(x_2;\\theta) \\cdots f(x_n;\\theta)&amp;=\\prod_{i=1}^n \\frac{1}{\\Gamma(2)\\theta^2} x_ie^{-x_i/\\theta}\\\\ &amp;=\\frac{1}{\\Gamma(2n)\\theta^{2n}} \\left( \\prod_{i=1}^n x_i \\right) e^{-\\sum x_i/\\theta}\\end{aligned}$$ 设 $$k_1[u(x_1,x_2,\\cdots,x_n);\\theta]=\\frac{1}{\\Gamma(2n)\\theta^{2n}}e^{-\\sum x_i/\\theta}, \\\\ \\quad k_2(x_1,x_2,\\cdots,x_n)=\\prod_{i=1}^n x_i$$ 其中$k_2$不依赖于$\\theta$，所以$Y_1$是关于$\\theta$的充分统计量. $\\square$ 性质1. 设$Y_1=u_1(X_1,X_2,\\cdots,X_n)$是$\\theta$的充分统计量，则 若$\\theta$存在唯一的最大似然估计$\\hat{\\theta}$，则$\\hat{\\theta}$是关于$Y_1$的函数； 若$u$为可逆函数，则$Z_1=u(Y_1)$仍是$\\theta$的充分统计量； (Rao-Blackwell) 设$Y_2=u_2(X_1,X_2,\\cdots,X_n)$是$\\theta$的一个无偏估计量，且$Y_2$不是以$Y_1$为单一变量的函数，则统计量$\\varphi(Y_1)=E[Y_2|Y_1]$是$\\theta$的无偏估计量，且满足$\\operatorname{var}[\\varphi(Y_1)] \\leq \\operatorname{var}[Y_2]$. 概念2: 完备分布族定义2. 设$Z$为连续型或离散型随机变量，其密度函数属于分布族$\\{h(z,\\theta):\\theta\\in\\Omega\\}$，称该分布族是完备的(complete)，如果对任意$\\theta\\in\\Omega$，$E[u(Z)]=0 \\Rightarrow u(z) \\equiv 0$ a.s. 定理2. 设$Y_1=u_1(X_1,X_2,\\cdots,X_n)$是$\\theta$的充分统计量，且$\\{g_1(y_1;\\theta):\\theta\\in\\Omega\\}$是完备分布族. 若$E[\\varphi(Y_1)]=E[\\psi(Y_1)]=\\theta$，则$\\varphi(Y_1)=\\psi(Y_1)$ a.s.，在此意义下，$\\varphi(Y_1)$是$\\theta$的唯一最小方差无偏估计. 例3 (ex 7.26). 考虑分布族$\\{h(z,\\theta)=\\frac{1}{\\theta},0&lt;z&lt;\\theta;\\theta \\in \\Omega\\}$ (a) 证明当$\\Omega=\\{\\theta:0&lt;\\theta&lt;\\infty\\}$时，该分布族是完备的； (b) 证明当$\\Omega=\\{\\theta:1&lt;\\theta&lt;\\infty\\}$时，该分布族不完备. 证明: (a) 设$Z \\sim h(z,\\theta)$，其中$h(z,\\theta)$属于该分布族. 不妨设$u(z)$为可积函数（事实上可测即可）且其原函数为$U(z)$，若$u(z)$满足 $$0=E[u(Z)]=\\int_0^{\\theta} u(z) \\cdot \\frac{1}{\\theta} \\mathrm{d}z, \\quad \\forall \\theta&gt;0$$ 等式两边同乘$\\theta$得到 $$0=\\int_{0}^{\\theta} u(z) \\mathrm{d}z = U(\\theta)-U(0), \\quad \\forall \\theta&gt;0$$ 于是$U(z) \\equiv C (\\forall , 0&lt;z&lt;\\theta) \\Rightarrow u(z)=U’(z)=0$ a.s. (b) 设$Z \\sim h(z,\\theta)$，其中$h(z,\\theta)$属于该分布族. 令 $$u(z) := \\left\\{\\begin{aligned}&amp;1 \\quad , 0&lt;z&lt;1/2 \\\\&amp;-1 , 1/2 \\leq z &lt; 1 \\\\&amp;0 \\quad , 1 \\leq z &lt; \\theta\\end{aligned}\\right.$$ 为非零值函数，则有 $$\\begin{aligned}E[u(Z)] &amp;= \\int_0^{\\theta} u(z) \\cdot \\frac{1}{\\theta} \\mathrm{d}z \\\\&amp;= \\frac{1}{\\theta} \\left( \\int_0^{1/2} 1 \\mathrm{d}z + \\int_{1/2}^{1} (-1) \\mathrm{d}z + \\int_1^{\\theta} 0 \\mathrm{d}z \\right) \\\\&amp;= \\frac{1}{\\theta} \\left( \\frac{1}{2}-\\frac{1}{2}+0 \\right) = 0 , \\quad \\forall 1&lt;\\theta&lt;\\infty\\end{aligned}$$ 由定义可知，该分布族不完备. $\\square$ 概念3: 指数分布族定义3. 设$\\Omega=(\\gamma,\\delta)$，连续型密度函数构成的分布族$\\{f(x;\\theta):\\theta\\in\\Omega\\}$称为指数分布族(exponential class)，如果$f(x;\\theta)$具有如下规范形式(regular case)： $$f(x;\\theta)=\\exp\\{p(\\theta)K(x)+S(x)+q(\\theta)\\}, \\quad x \\in \\mathscr{A}=(a,b)$$ 其中 (1) $\\mathscr{A}=(a,b)$与$\\theta$无关； (2) $p(\\theta)$是$\\theta$的非平凡（即$p(\\theta) \\not\\equiv C$）连续函数； (3) $K’(x) \\not\\equiv 0$和$S(x)$都是$x$的连续函数. 定义4. 设$\\Omega=(\\gamma,\\delta)$，离散型密度函数构成的分布族$\\{f(x;\\theta):\\theta\\in\\Omega\\}$称为指数分布族，如果$f(x;\\theta)$具有如下规范形式： $$f(x;\\theta)=\\exp\\{p(\\theta)K(x)+S(x)+q(\\theta)\\}, \\quad x=a_1,a_2,\\cdots$$ 其中 (1) $\\{x:x=a_1,a_2,\\cdots\\}$与$\\theta$无关； (2) $p(\\theta)$是$\\theta$的非平凡连续函数； (3) $K(x)$是$x$的非平凡函数. 定理3. 若随机样本$X_1,X_2,\\cdots,X_n$的密度函数$f(x;\\theta),\\gamma&lt;\\theta&lt;\\delta$属于指数分布族且具有规范形式 $$f(x;\\theta)=\\exp\\{p(\\theta)K(x)+S(x)+q(\\theta)\\}$$ 则$Y_1=\\sum_{i=1}^n K(X_i)$是$\\theta$的充分统计量，且$\\{g_1(y_1;\\theta),\\gamma&lt;\\theta&lt;\\delta\\}$是完备分布族，即$Y_1$是$\\theta$的充分完备统计量(complete sufficient statistic). 推论4. 设$Y_1$是$\\theta$的充分完备统计量，若$\\varphi(Y_1)$满足$E[\\varphi(Y_1)]=\\theta$，则$\\varphi(Y_1)$是$\\theta$唯一最小方差无偏估计. 例4 (ex 7.39). 设随机样本$X_1,X_2,\\cdots,X_n, ; n&gt;2$服从二项分布$b(1,\\theta)$. (a) 证明$Y_1=\\sum X_i$为$\\theta$的充分完备统计量； (b) 求$\\theta$的一致最小方差无偏估计$\\varphi(Y_1)$； (c) 令$Y_2=(X_1+X_2)/2$，计算$E[Y_2]$； (d) 计算$E[Y_2|Y_1=y_1]$. 证明: (a) 由于$X_i$的密度函数 $$f(x;\\theta)=\\theta^x(1-\\theta)^{1-x}=\\exp\\Bigg\\{ \\overbrace{\\ln \\frac{\\theta}{1-\\theta}}^{p(\\theta)} \\cdot \\underbrace{x}_{K(x)} + \\overbrace{0}^{S(x)} + \\underbrace{\\ln(1-\\theta)}_{q(\\theta)} \\Bigg\\}, \\quad x=0,1$$ 为满足以上(1)(2)(3)条件的规范形式，所以$\\{f(x;\\theta)\\}$为指数分布族，于是$Y_1=\\sum K(X_i)=\\sum X_i$为$\\theta$的充分完备统计量. (b) 由于$Y_1 \\sim b(n,\\theta)$，所以$E[Y_1]=n\\theta$. 于是当我们令$\\varphi(Y_1)=\\frac{1}{n}Y_1$，则$E[\\varphi(Y_1)]=\\theta$，且$\\varphi(Y_1)$为$\\theta$的一致最小方差无偏估计. (c) $E[Y_2]=\\frac{E[X_1]+E[X_2]}{2}=\\frac{\\theta+\\theta}{2}=\\theta$. (d) 由于$Y_1$为充分统计量，$Y_2$为无偏估计量，由Rao-Blackwell定理可知$E[Y_2|Y_1=y_1] \\equiv \\psi(y_1)$也是无偏估计量，即$E[\\psi(y_1)]=\\theta$. 于是显然$\\psi(y_1)=\\varphi(y_1)=\\frac{1}{n}y_1$，从而$E[Y_2|Y_1=y_1]=\\psi(y_1)=\\frac{1}{n}y_1$. $\\square$ 定理5. 若随机样本$X_i$的密度函数$f(x;\\theta)$满足规范形式，则 $$E[K(X_i)] = -\\frac{q’(\\theta)}{p’(\\theta)},$$ 如果设$Y_1=\\sum K(X_i)$，则 $$E[Y_1]=\\sum E[K(X_i)]=-n\\frac{q’(\\theta)}{p’(\\theta)}.$$ 事实上，这直接给出了指数分布族中充分完备统计量的数学期望. 总结Rao-Blackwell定理告诉我们可以利用充分统计量使无偏估计量的方差逐步缩减，但实际上我们并不需要从无偏估计量出发，而是通过构造充分完备统计量直接求得UMVUE，另外指数分布族的引入也极大地方便了充分完备统计量的构造. 二、有效估计量定义4. 随机变量$X$的Fisher信息量(Fisher information)定义为 $$I(\\theta)=E\\left[\\left(\\frac{\\partial \\ln f(X;\\theta)}{\\partial \\theta}\\right)^2\\right]=-E\\left[\\frac{\\partial^2 \\ln f(X;\\theta)}{\\partial \\theta^2}\\right]$$ 随机样本$(X_1,X_2,\\cdots,X_n)$的Fisher信息量定义为 $$I_n(\\theta)=nI(\\theta)$$ 一般来说，上述第二种表达式更容易计算. 定理6 (Rao-Cramer inequality). 设$Y=u(X_1,X_2,\\cdots,X_n)$是$\\theta$的无偏估计量，则$Y$的方差$\\sigma_Y^2:=\\operatorname{var}[Y]$满足 $$\\sigma_Y^2=\\operatorname{var}[Y] \\geq \\frac{1}{nI(\\theta)}$$ 特别地，若$Y$的方差恰好为Rao-Cramer下界，即 $$\\sigma_Y^2=\\operatorname{var}[Y] = \\frac{1}{nI(\\theta)}$$ 则$Y$是$\\theta$的一致最小方差无偏估计量. 定义5. 设$Y$是$\\theta$的无偏估计量，称$Y$是$\\theta$的有效估计量(efficient estimator)，若$Y$的方差达到Rao-Cramer下界，即 $$\\sigma_Y^2=\\operatorname{var}[Y] = \\frac{1}{nI(\\theta)}$$ 注： {有效估计量} $\\subset$ {UMVUE} 定义6. 设$Y$是$\\theta$的无偏统计量，定义其功效(efficiency)为Rao-Cramer下界 $\\frac{1}{nI(\\theta)}$与$Y$的方差$\\sigma_Y^2$的比值，即 $$\\text{efficiency of } Y := \\frac{1}{nI(\\theta) \\cdot \\sigma_Y^2} \\leq 1$$ 特别地，$Y$为有效估计量，当且仅当$\\text{efficiency of } Y = 1$. 例5 (ex 8.15). 设$X \\sim \\Gamma(4,\\theta)$，其中$\\theta&gt;0$. (a) 求Fisher信息量$I(\\theta)$； (b) 设随机变量$X_1,X_2,\\cdots,X_n$服从该分布，证明$\\theta$的最大似然估计为有效估计量. 证明: (a) 由于$X \\sim \\Gamma(4,\\theta)$，则有 $$\\begin{aligned}f(x;\\theta)=\\frac{1}{6\\theta^4} x^3 e^{-\\frac{x}{\\theta}}&amp;\\Rightarrow \\ln f(x;\\theta)=3\\ln x-\\frac{x}{\\theta}-\\ln \\Gamma(4)-4\\ln \\theta \\\\&amp;\\Rightarrow \\frac{\\partial \\ln f(x;\\theta)}{\\partial \\theta} = \\frac{x}{\\theta^2} - \\frac{4}{\\theta} \\\\&amp;\\Rightarrow \\frac{\\partial^2 \\ln f(x;\\theta)}{\\partial \\theta^2} = -\\frac{2x}{\\theta^3}+\\frac{4}{\\theta^2}\\end{aligned}$$ 所以$I(\\theta)=\\frac{2}{\\theta^3}E[X]-\\frac{4}{\\theta^2}=\\frac{4}{\\theta^2}$. (b) 易求得$\\theta$的最大似然估计为$\\hat{\\theta}=\\frac{1}{4}\\overline{X}$，由于 $$\\operatorname{var}[\\hat{\\theta}] = \\frac{1}{16} \\operatorname{var}[\\overline{X}] = \\frac{1}{16} \\cdot \\frac{4\\theta^2}{n} = \\frac{\\theta^2}{4n} = \\frac{1}{nI(\\theta)}$$ 所以$\\hat{\\theta}$为$\\theta$的有效估计量. $\\square$ 参考文献 Hogg, R. V., &amp; Craig, A. T. (. T. (1995). Introduction to mathematical statistics (5th ed.). Englewood Cliffs, N.J.: Prentice Hall.","link":"/2020/01/13/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/UMVUE/"}],"tags":[{"name":"数理统计","slug":"数理统计","link":"/tags/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"}],"categories":[{"name":"数学","slug":"数学","link":"/categories/%E6%95%B0%E5%AD%A6/"},{"name":"数理统计","slug":"数学/数理统计","link":"/categories/%E6%95%B0%E5%AD%A6/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"}]}